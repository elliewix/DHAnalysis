{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from scrapy.selector import Selector\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'abstractpage.txt'\n",
    "file(filename, \"wb\").write(urllib2.urlopen('http://dh2014.org/program/abstracts/').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('abstractpage.txt','rt') as abpage:\n",
    "    body = abpage.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pages = Selector(text=body).xpath('//a[starts-with(@href,\\'http://dharchive.org/paper/DH2014/\\')]/@href').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    filename = page.split('/')[-1]\n",
    "    file(\"abstracts/\" + filename, \"wb\").write(urllib2.urlopen(page).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getFiles(path):\n",
    "    \"\"\"Function to return a list of all files within a folder\"\"\"\n",
    "    files = [ f for f in listdir(path) if isfile(join(path,f)) and f[0] != '.' ]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['Poster', 'Paper', 'Workshops', 'Plenary', 'Panel'])\n"
     ]
    }
   ],
   "source": [
    "types = set([fname.split('-')[0] for fname in getFiles(\"abstracts\")])\n",
    "print types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panel-932.xml curation\n",
      "Paper-101.xml curation\n",
      "Paper-134.xml javascript\n",
      "Paper-134.xml curation\n",
      "Paper-137.xml python\n",
      "Paper-167.xml curation\n",
      "Paper-171.xml python\n",
      "Paper-196.xml javascript\n",
      "Paper-199.xml java\n",
      "Paper-200.xml python\n",
      "Paper-205.xml java\n",
      "Paper-224.xml python\n",
      "Paper-224.xml javascript\n",
      "Paper-294.xml javascript\n",
      "Paper-330.xml javascript\n",
      "Paper-373.xml javascript\n",
      "Paper-416.xml javascript\n",
      "Paper-468.xml curation\n",
      "Paper-480.xml curation\n",
      "Paper-483.xml curation\n",
      "Paper-492.xml curation\n",
      "Paper-504.xml curation\n",
      "Paper-63.xml python\n",
      "Paper-639.xml java\n",
      "Paper-676.xml curation\n",
      "Paper-68.xml curation\n",
      "Paper-680.xml javascript\n",
      "Paper-743.xml curation\n",
      "Paper-756.xml java\n",
      "Paper-758.xml curation\n",
      "Paper-778.xml python\n",
      "Paper-796.xml javascript\n",
      "Paper-803.xml python\n",
      "Paper-805.xml javascript\n",
      "Paper-823.xml curation\n",
      "Paper-866.xml curation\n",
      "Paper-877.xml python\n",
      "Paper-877.xml java\n",
      "Paper-888.xml python\n",
      "Paper-888.xml javascript\n",
      "Paper-892.xml javascript\n",
      "Paper-924.xml java\n",
      "Paper-925.xml java\n",
      "Poster-219.xml java\n",
      "Poster-219.xml curation\n",
      "Poster-256.xml javascript\n",
      "Poster-290.xml javascript\n",
      "Poster-427.xml curation\n",
      "Poster-48.xml java\n",
      "Poster-651.xml curation\n",
      "Poster-735.xml curation\n",
      "Poster-782.xml java\n",
      "Poster-834.xml curation\n",
      "Poster-842.xml java\n",
      "Workshops-902.xml curation\n",
      "Workshops-904.xml python\n",
      "Workshops-904.xml javascript\n",
      "Workshops-904.xml curation\n",
      "Workshops-906.xml curation\n",
      "Workshops-908.xml curation\n",
      "Workshops-910.xml curation\n",
      "Workshops-912.xml java\n",
      "375 total\n",
      "10 total python\n",
      "15 total javascript\n",
      "12 total java\n",
      "25 total curation\n"
     ]
    }
   ],
   "source": [
    "removethings = ['<em>','</em>','<b>','</b>','<i>','</i>']\n",
    "\n",
    "totalconfthings = 0\n",
    "totalpython = 0\n",
    "totaljavascript = 0\n",
    "totaljava = 0\n",
    "totalcuration = 0\n",
    "\n",
    "for ab in getFiles(\"abstracts/\"):\n",
    "    name = ab.split('.')[0]\n",
    "    with open(\"abstracts/\" + ab,'rb') as abfile:\n",
    "        body = abfile.read()\n",
    "    for each in removethings:\n",
    "        body = body.replace(each, '')\n",
    "    text = \"\\n\".join(Selector(text=body).xpath('//p//text()').extract())\n",
    "    if ab.endswith('.xml'):\n",
    "        totalconfthings += 1\n",
    "        if 'python' in text.lower():\n",
    "            totalpython += 1\n",
    "            print ab, 'python'\n",
    "        if 'javascript' in text.lower():\n",
    "            totaljavascript += 1\n",
    "            print ab, 'javascript'\n",
    "        elif 'java' in text.lower():\n",
    "            totaljava += 1\n",
    "            print ab, 'java' # there is some weakness in how I'm doing this calculation.  TODO: fix that\n",
    "        if 'curation' in text.lower():\n",
    "            print ab, 'curation'\n",
    "            totalcuration += 1\n",
    "    with open(\"abstracts/\" + name + \".txt\", 'wt') as about:\n",
    "        about.write(text.encode('utf8'))\n",
    "        \n",
    "print totalconfthings, \"total\"\n",
    "print totalpython, \"total python\"\n",
    "print totaljavascript, \"total javascript\"\n",
    "print totaljava, \"total java\"\n",
    "print totalcuration, \"total curation\"\n",
    "\n",
    "    #print text.lower()\n",
    "#     if \"humanities\" in text.lower():\n",
    "#         print \"python found in\", ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \"\".join(Selector(text=body).xpath('//p//descendant::text()').extract())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### clean out the text punctuation crap\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "xmls = [xml for xml in getFiles(\"abstracts/\") if xml.endswith(\".xml\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#xmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(\"abstracts/\" + 'Paper-208.xml','rb') as abfile:\n",
    "    body = abfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'In a recent overview of issues in the digital humanities, Manfred Thaller notes the importance of: \\u201c(1) access to the information needed to tackle a research question, (2) the analysis of that information by tools reflecting the methodological requirements of the specific discipline and research problem and (3) the publication of the new information gained by the analytical process.\\u201d (Thaller 2012:11)', u'1', u\"For most of the world's 7000 languages there are few records available via the internet. Efforts to increase the documentation of these small languages have led to the development of tools and repositories over the past decade. I suggest that Thaller\\u2019s desiderata are reflected in the language documentation activities of the creation of archives, metadata systems and the ability to locate, store, retrieve and re-use language records. The network of language archives represented by the Open Language Archives Community (OLAC) has adopted a common metadata system that each archive serves for OLAC\\u2019s aggregation, allowing more specific searches than can be provided by google, for example. However, not all digital language archives currently provide metadata to OLAC, rendering their collections invisible to the aggregated search. While their webpages may be accessible to web- searches, they do not allow the targeted search by language that is the focus of OLAC\\u2019s aggregator. Other repositories (including many institutional repositories\\u2014national libraries and archives, mission archives and so on) have language content that is not noted in the collection\\u2019s catalog, and the catalog may not be available for web-harvesting. Finally, there are collections still held by their creators and not in a repository at all.\", u'This paper discusses two approaches to making collections of primary language material locatable and accessible. While the methods are generalisable to any discipline, this paper describes an index of records of language material for collections that have no such metadata and for which no other mechanism is foreseeable. The first approach builds a traceable index of a researcher\\u2019s discoveries in existing repositories, for example, a state library or archive, using established aggregation services. The second is a survey that aims to locate and digitise smaller collections that are currently outside established institutions, typically still in the care of the researcher.', u'The language index provides metadata in an Open Archives Initiative-compliant form, allowing records', u'2', u' to be found in generic language searches', u'3', u'. Not all repositories can provide metadata using ISO-639-3 language codes, so it is useful to provide a mechanism whereby researchers can build this resource as they discover new material. In general, repositories are just unaware of standards rather than being reluctant to share data, hence the need for them to either change their metadata system (which is unlikely) or for an index of the kind described here, that points to their collections.', u'The paper will demonstrate the index as it is currently implemented in the catalog of the Pacific and Regional Archive for Digital Sources in Endangered Cultures (PARADISEC), and will discuss the issue of persistence of the links provided and future possible alternatives to exposing collections that are otherwise invisible to aggregation. ', u'In an effort to locate what I have called invisible collections I launched a survey', u'4', u' in 2012 asking respondents to identify language collections that needed to be either (or both) digitised or described using OLAC\\u2019s service. I tried to keep the questions as simple and easy to answer as possible. In this paper I will report on the findings of this survey, in particular noting that they point to the need for training; for simple metadata entry tools; for standards-compliant metadata repositories; and for recognition of collections of primary material as a form of scholarly output. ', u'The survey questions were as follows:', u'1. Do you know of recordings of small or endangered languages that are not yet digitised? These could be in personal collections or in established repositories that do not plan to digitise their collections. If so, please provide as much detail as you can about the number and type of recordings (reel to reel, cassette, DAT etc), the content, and the state of their current storage. Can you provide information about who to contact about these collections?', u'2. Do you know of collections whose catalogs are not available through federated searches (that is, they are only available if you visit their website and not anywhere else on the web) and for which we could provide a reference to make it easier to find them?\\xa0', u'3. Do you know of repositories of manuscripts that have received little attention from linguists but which are likely, in your opinion, to have linguistic records in them? These may include, for example, missionary archives or State administrative archives.', u'4. Please include your name and contact email so we can follow up with you if necessary (email addresses will not be added to any lists). (Please indicate if you allow us to publish an anonymised version of your response).', u'The survey form was publicised among linguistic networks. It is now nominated as a future activity of the international network of language archives, DELAMAN', u'5', u' which should ensure wider coverage. As a first step, it has revealed an interesting variety of collections, each with characteristics that are significant for the effort of making such collections available. At a time when funding for digitisation is difficult to obtain, it is important to recognise that unique cultural heritage recordings such as these are at risk of being lost. A summary of some responses and an observation about the broader significance of each is given below.', u'(1) 22 tapes of a Sudanese language held in Washington DC by a retired linguist \\u2013 how to get them digitised and where to store them then? 22 tapes are sort of manageable. There are also a large number of notes that need to be scanned. For a retired researcher it may not be easy to access the equipment needed to do this work. ', u'(2) Several hundred cassettes in a Solomon Islands language, particularly valuable as they are recorded by a speaker, so capturing lots of natural speech. Digitising such a collection is a serious undertaking needing significant funds.', u'(3) The tapes are in Stockholm, stored in a box but the recorder is based in Chicago and is still an active academic. A basic problem of access of the collector to their own material.', u'(4) Colorado, USA, a dozen reel-to-reel and two dozen cassette tapes with a senior linguist concerned to make the collection safe and not being sure what to do.', u'(5) Tapes were deposited with a national Cultural Centre in a small Pacific country that may or may not have the resources to look after them. It does not publish its catalog (if it actually has one) so it is not clear if these tapes need to be digitised or not, or what conditions may be placed on access to them.', u'(6) A recent MA in Linguistics at one of the PARADISEC consortium universities, tapes stored in boxes. Paper transcripts may have been thrown out. Shows lack of communication even within our own departments.', u'(7) A collection of [language] tapes stored in a Harvard University repository which may not prioritise digitising it (but could if funding were made available). ', u'(8) Researchers who have small collections, less than twenty tapes, and digitise them themselves by connecting a tape player to a digital recorder. Problem of methods used in digitisation, may damage the tape and not result in the best digital file.', u'One reason that these collections are not digitised is clearly the lack of importance placed by academia on the re-use of primary research materials. If it were an acknowledged research output to create archived and accessible collections of primary data, counting towards promotion and tenure, then it is more likely that cases like those listed above would no longer occur. It is clear that much remains to be done to extend the reach of digital language archives, assisting in locating legacy collections, describing and digitising them, connecting with source communities/individuals, creating a means for online annotation (crowdsourcing) and of valuing the collections (both monetarily or academically). I conclude by discussing an online service for providing small metadata snippets pointing to these otherwise invisible collections. This paper presents these efforts based around the digital archive Pacific and Regional Archive for Digital Sources in Endangered Cultures (PARADISEC).', u'Thaller, Manfred', u' (Ed.). (2012). ', u'Controversies around the Digital Humanities', u'. Historical Social Research Vol. 37 (2012), No. 3.', u'www.language-archives.org/item/oai', u':paradisec.org.au:JL1-link', u'www.language-archives.org/item/oai', u':paradisec.org.au:JL1-link', u'\\n    ', u'www.paradisec.org.au/PDSCSurvey.html', u'\\n  ', u'\\n    ', u'www.delaman.org', u'\\n  ']\n"
     ]
    }
   ],
   "source": [
    "found = Selector(text=body).xpath('//p//descendant::text()').extract()\n",
    "\n",
    "for each in Selector(text=body).xpath('//p//descendant::text()').extract():\n",
    "    pass\n",
    "\n",
    "print found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
