<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><title>DHArchive</title><meta http-equiv="content-type" content="text/html; charset=utf-8" /><link rel="stylesheet" type="text/css" href="/s/dharchive.css"></link><link rel="shortcut icon" type="image/x-icon" href="favicon.ico" /></head><body><header><a href="/"><img src="/i/logo.png" id="logo" /><h1><acronym>dharchive</acronym>.org</h1></a></header><section><a href="/?c=DH2014"><img src="/data/figures/DH2014/banner.jpg" class="banner"/></a><div id="read"><aside><a class="btn" href="javascript:window.print();"><img src="/i/print.png"/> Print</a><a class="btn" href="/data/xml/DH2014/Poster-36.xml"><img src="/i/download.png"/> XML</a></aside><?xml version="1.0" encoding="utf-8"?>
<article xmlns="http://www.tei-c.org/ns/1.0" xmlns:xhtml="http://www.w3.org/1999/xhtml" xmlns:tei="http://www.tei-c.org/ns/1.0">
  <header xmlns="">
    <h1>Mapping French Press to the Digital Age</h1>
    <ul id="details">
      <li><label>Category:</label>Poster</li>
      <li><label>Session:</label>2</li>
      <li><label>Date:</label>2014-07-10<label>Time:</label>16:00:00</li>
      <li><label>Room:</label>Amphipôle Common Area</li>
    </ul>
    <ul id="authors">
      <li>
        <a href="mailto:alaa.abi-haidar@lip6.fr"><span class="author-surname">Abi Haidar</span>,
									<span class="author-forename">Alaa</span></a>
        <a href="http://www.google.com/#q=Abi Haidar, Alaa">
          <img src="/i/search.png"/>
        </a>
        <span class="author-affiliation">Univeristy of Pierre and Marie Curie</span>
      </li>
      <li>
        <a href="mailto:jean-gabriel.ganascia@lip6.fr"><span class="author-surname">Ganascia</span>,
									<span class="author-forename">Jean-Gabriel</span></a>
        <a href="http://www.google.com/#q=Ganascia, Jean-Gabriel">
          <img src="/i/search.png"/>
        </a>
        <span class="author-affiliation">Univeristy of Pierre and Marie Curie</span>
      </li>
    </ul>
  </header>
  <p xmlns=""/>
  <h2 xmlns="">Abstract</h2>
  <p xmlns="">The unsupervised use of dictionary-lookup is known to enhance NER, however dictionaries have limitations for being finite and ambiguous. On the other hand, supervised NER such as Stanford's NER Classifier that we tested here is known to perform very well but only with the availability of huge amounts of manually annotated training data that is very costly, time consuming and sometimes inaccurate due to inter-annotator inconsistencies. Therefore, we develop and discuss an original unsupervised approach for Named Entity Recognition (NER) and Disambiguation (UNERD) using a French knowledge-base and a statistical contextual disambiguation technique that slightly outperformed Stanford's NER Classifier (when trained on a small portion of manually annotated data) and Aleda's dictionary lookup. Furthermore, we devise a solution to identify and highlight named entity tagging on the original scanned images thus preserving the newspaper’s layout and feel. </p>
  <h2 xmlns="">1. Introduction</h2>
  <p xmlns="">Huge amounts of printed manuscripts from old French journals (from the 19th and 20th century) have been recently digitized and published by the National French Library, la Bibliotheque Nationale Francaise (BnF). However, the massive amounts of produced textual data are highly unstructured and hard to index or search, needless to mention the digitization errors resulting from ill-preserved or damaged manuscripts and imperfect Optical Character Recognition (OCR) techniques.</p>
  <p xmlns="">Named Entity Recognition (NER) is a task of information extraction that aims to identify in-text references to concepts such as people, locations and organizations, mainly in unstructured natural-language text. NER is very useful for text indexing, text summarization, question answering and several other tasks that enhance the experience between humans and literature. Furthermore, advanced NER and disambiguation techniques are capable of dealing with noise resulting from digitization errors.</p>
  <p xmlns="">Several supervised learning techniques, such as Stanford’s Conditional Random Field (CRF) Classifier <cite href="#n1">1</cite>, have been developed to address the question of NER very successfully, however they require large manually-annotated corpora of text, which is very expensive to obtain and maintain. On the other hand, with the abundance of publicly accessible knowledge bases and dictionaries, such as Freebase, geonames and Aleda <cite href="#n2">2</cite>, unsupervised methods have become popular especially since they require no pre-annotated training data. However,  several challenges have arisen from the choice of appropriate knowledge bases, resolving homonymous ambiguity, detecting entity boundaries, and identifying less common name entities. Moreover, most studies have been dedicated to English text while text in other languages, that has recently been digitized and published at astounding rates, has received little or no attention.</p>
  <div xmlns="" class="figure">
    <img src="/data/figures/DH2014/DH2014_30_DH2014_30_f1.highres"/>
    <figcaption>
      <p>Fig. 1: Original scanned image from "Le Petit Parisen"</p>
    </figcaption>
  </div>
  <p xmlns="">Here, we discuss our model of Unsupervised Named Entity Recognition and Disambiguation (UNERD), and validate it on digitized French text from the 19th century. We claim that NER using Knowledge based disambiguation is especially relevant for minimally annotated texts that are expensive to annotate in several languages and domains, which is often the case in Digital Humanities. More details about our algorithm and its performance are detailed in <cite href="#n3">3</cite>. We also discuss a solution that uses the XML digitized data in order to preserve the location of entities and highlight them on the original scanned image as well as in the OCRised text. </p>
  <h2 xmlns="">2. Data</h2>
  <p xmlns="">Here we discuss the data at hand, its format, encoding and necessary text processing. Next we discuss a portion of annotated data that is used for validation. Finally, we propose a solution that allows tagging the originally scanned journal image.</p>
  <h2 xmlns="">2.1. Corpus</h2>
  <p xmlns="">The corpus consists of recently digitized and published old unlabelled French journals. More specifically, the data consists of a subset of “Le Petit Parisien” journal supplied by Bibliotheque nationale de France (BnF). It was originally published between 1863 and 1944 with a total of 29616 issues. The corpus we are using comprises 260 issues with a total of 1098 pages of natural French text. The pages were recently digitized and OCRised and encoded in the ALTO format, which is an open XML standard for representing OCR text. Both scanned pages and text formats of “Le Petit Parisien” are accessible at the BnF website via their digital portal, Gallica: <a href="http://gallica.bnf.fr/ark:/12148/cb34419111x/date.langFR">gallica.bnf.fr/ark:/12148/cb34419111x/date.langFR</a></p>
  <h2 xmlns="">2.2. Format</h2>
  <p xmlns="">The ALTO file has 3 major sections:</p>
  <p xmlns="">&lt;Description&gt; contains the metadata about the ALTO file.</p>
  <p xmlns="">&lt;Styles&gt; specify the text and paragraph styles.</p>
  <p xmlns="">&lt;Layout&gt; describes the main content subdivided into &lt;page&gt; elements.</p>
  <p xmlns="">Each page of the content is further divided into margins and printspace, each of which containing  lines, images or textblocks.  The ALTO format provides OCR confidence for each word. The XML files are encoded using the iso-8859-1 encoding. Handling this dataset is challenging due to the old ill-preserved manuscripts resulting in many OCR errors. However, we exclude text blocks with low OCR confidence (lower than 0.85).</p>
  <h2 xmlns="">2.3. Validation Data</h2>
  <p xmlns="">The corpus of French journals is very expensive and time consuming to annotate. Therefore we had experts annotate only a small portion of 4171 words of which the numbers annotated entities are 75 Person, 78 Location, and 22 Organisation entities.</p>
  <h2 xmlns="">2.4. Text Processing</h2>
  <p xmlns="">Text preprocessing is essential for decoding and analysing the data and  the first step is to prepare OCR text for entity extraction such as converting the XML iso-8859-1 encoded corpus into standard UTF-8 encoded text, excluding text-blocks with confidence less than 85% or containing words with length more than 15 characters, applying Part of Speech Tagging (using TreeTagger) and removing French stop-words.</p>
  <h2 xmlns="">2.5. Representation</h2>
  <p xmlns="">Here, we propose a solution that enables the ALTO XML format to keep track of the location of the words in order to highlight the entities not only for the OCRised text but also for on the original scanned image thus maintaining the layout, font types and feel or reading a newspaper.</p>
  <p xmlns="">The ALTO XML format preserves the location of all OCRised words using the tags HPOS and VPOS as illustrated in the following code for the text “Toussaint, humide et pluvieux;”:</p>
  <div xmlns="" class="figure">
    <img src="/data/figures/DH2014/DH2014_30_capture1.jpg"/>
    <figcaption>
      <p>Fig. 2: Sample ALTO XML code representing 4 words and their positions on the scanned image</p>
    </figcaption>
  </div>
  <div xmlns="" class="figure">
    <img src="/data/figures/DH2014/DH2014_30_f2.highres"/>
    <figcaption>
      <p>Fig. 3: Sample original scanned image from "Le Petit Parisien" highlighting entities corresponding to the following categories: Location (blue), Organization (green) and Person (red) names.</p>
    </figcaption>
  </div>
  <h2 xmlns="">3. Method</h2>
  <p xmlns="">Our algorithm first uses a French knowledge-base, namely Aleda, to find contextual collocations (or word co-occurrences) for each entity class (Person, Location or Organisation) within a window of size n.  Next, we compare the context of the candidate entity with the classes'contextual cues that eventually suggest a class name for each contextual word around the entity-mention. The comparison may or may not rely on the position of the three neighboring words that make its context based on the following disambiguation techniques.</p>
  <ul xmlns="">
    <li>The single significant cue selects the class of the contextual cue with the highest TF-IDF score.</li>
    <li>The bag of words selects the class of the contextual cue  with the highestTF-IDF ignoring the relative positions of the contextual cues.</li>
    <li>The majority rule selects the class that has the majority of the votes by contextual cues or at least two out of three votes.</li>
  </ul>
  <p xmlns="">Finally, our algorithm uses the majority rule to select a class based on the results from the previous techniques i.e. if at least two disambiguation techniques agree on a class then this class is chosen. Furthermore, the entity-boundary detection uses a Parts of Speech (PoS) tagger that identifies noun phrase boundaries. </p>
  <h2 xmlns="">3. Results</h2>
  <p xmlns="">We tested our UNERD algorithm on a small portion of data that we had annotated by experts and we compared it using k-fold cross-validation to mere dictionary look-up, namely Aleda, and to Stanford’s NER Conditional Random Field (CRF) Classifier. The preliminary results reported using F-score are clearly in favor of our unsupervised method as shown in table 1. </p>
  <table xmlns="">
    <tr>
      <td> F-score</td>
      <td> Aleda Look-up</td>
      <td> Stanford NER</td>
      <td> UNERD</td>
    </tr>
    <tr>
      <td> LOC</td>
      <td> 0.33</td>
      <td> 0.54</td>
      <td> 0.77</td>
    </tr>
    <tr>
      <td> PER</td>
      <td> 0.73</td>
      <td> 0.75</td>
      <td> 0.83</td>
    </tr>
    <tr>
      <td> ORG</td>
      <td> 0.20</td>
      <td> 0.44</td>
      <td> 0.46</td>
    </tr>
    <tr>
      <td> AVERAGE</td>
      <td> 0.42</td>
      <td> 0.58</td>
      <td> 0.69</td>
    </tr>
  </table>
  <p xmlns="">For more details about K-fold cross-validation, F-score please refer to <cite href="#n4">4</cite>.</p>
  <h2 xmlns="">Conclusion</h2>
  <p xmlns="">Here, discussed an original unsupervised named entity recognition and disambiguation approach which outperforms mere dictionary lookup and supervised learning on small portions of annotated data. Arguably, Stanford’s supervised NER can outperform our method if trained on a larger set of manually annotated data. However, that may be true with the availability of huge amounts of annotated data that are very expensive and time consuming to produce.  Our NER and disambiguation technique is statistical and is supposed to work on various languages and domains using no annotated data. We also discussed an approach for keeping track and making use of the ALTO XML format in order to highlight entities on the original scanned image. </p>
  <h2 xmlns="">References</h2>
  <p xmlns="">1. <strong>Finkel, Jenny Rose, Trond Grenager, and Christopher Manning</strong> (2005). <em>Incorporating non-local information into information extraction systems by gibbs sampling.</em> Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics.
					</p>
  <p xmlns="">2. <strong>Benoit Sagot, Rosa Stern, et al.</strong> (2012). <em>Aleda, a free large-scale entity database for french</em>. Proceedings of LREC 2012
					</p>
  <p xmlns="">3. <strong>Mosallem, Yusra, Abi-Haidar, Alaa and Ganascia Jean-Gabriel</strong> (Submitted). <em>Unsupervised Named Entity Recognition and Disambiguation: An Application to Old French Journals</em>.
					</p>
  <p xmlns="">4. <strong>Feldman, Ronen, and James Sanger</strong> (2007). <em>The text mining handbook: advanced approaches in analyzing unstructured data</em>. Cambridge University Press
					</p>
</article>
</div></section><footer><hr/></footer></body></html>