<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><title>DHArchive</title><meta http-equiv="content-type" content="text/html; charset=utf-8" /><link rel="stylesheet" type="text/css" href="/s/dharchive.css"></link><link rel="shortcut icon" type="image/x-icon" href="favicon.ico" /></head><body><header><a href="/"><img src="/i/logo.png" id="logo" /><h1><acronym>dharchive</acronym>.org</h1></a></header><section><a href="/?c=DH2014"><img src="/data/figures/DH2014/banner.jpg" class="banner"/></a><div id="read"><aside><a class="btn" href="javascript:window.print();"><img src="/i/print.png"/> Print</a><a class="btn" href="/data/xml/DH2014/Paper-819.xml"><img src="/i/download.png"/> XML</a></aside><?xml version="1.0" encoding="utf-8"?>
<article xmlns="http://www.tei-c.org/ns/1.0" xmlns:xhtml="http://www.w3.org/1999/xhtml" xmlns:tei="http://www.tei-c.org/ns/1.0">
  <header xmlns="">
    <h1>Swiss Voice App: A smartphone application for crowdsourcing Swiss German dialect data</h1>
    <ul id="details">
      <li><label>Category:</label>Short Paper</li>
      <li><label>Session:</label>2</li>
      <li><label>Date:</label>2014-07-09<label>Time:</label>11:00:00</li>
      <li><label>Room:</label>315 - Amphipôle</li>
    </ul>
    <ul id="authors">
      <li>
        <a href="mailto:"><span class="author-surname">Kolly</span>,
									<span class="author-forename">Marie-José</span></a>
        <a href="http://www.google.com/#q=Kolly, Marie-José">
          <img src="/i/search.png"/>
        </a>
        <span class="author-affiliation">University of Zurich, Switzerland</span>
      </li>
      <li>
        <a href="mailto:"><span class="author-surname">Leemann</span>,
									<span class="author-forename">Adrian</span></a>
        <a href="http://www.google.com/#q=Leemann, Adrian">
          <img src="/i/search.png"/>
        </a>
        <span class="author-affiliation">University of Zurich, Switzerland</span>
      </li>
      <li>
        <a href="mailto:"><span class="author-surname">Dellwo</span>,
									<span class="author-forename">Volker</span></a>
        <a href="http://www.google.com/#q=Dellwo, Volker">
          <img src="/i/search.png"/>
        </a>
        <span class="author-affiliation">University of Zurich, Switzerland</span>
      </li>
      <li>
        <a href="mailto:"><span class="author-surname">Goldman</span>,
									<span class="author-forename">Jean-Philippe</span></a>
        <a href="http://www.google.com/#q=Goldman, Jean-Philippe">
          <img src="/i/search.png"/>
        </a>
        <span class="author-affiliation"> University of Geneva, Switzerland </span>
      </li>
      <li>
        <a href="mailto:"><span class="author-surname">Hove</span>,
									<span class="author-forename">Ingrid</span></a>
        <a href="http://www.google.com/#q=Hove, Ingrid">
          <img src="/i/search.png"/>
        </a>
        <span class="author-affiliation">University of Zurich, Switzerland</span>
      </li>
      <li>
        <a href="mailto:"><span class="author-surname">Almajai</span>,
									<span class="author-forename">Ibrahim</span></a>
        <a href="http://www.google.com/#q=Almajai, Ibrahim">
          <img src="/i/search.png"/>
        </a>
        <span class="author-affiliation"> University of Geneva, Switzerland </span>
      </li>
    </ul>
  </header>
  <p xmlns=""/>
  <p xmlns=""/>
  <p xmlns=""/>
  <p xmlns=""/>
  <p xmlns=""/>
  <p xmlns=""/>
  <h2 xmlns="">
    <strong>1 Introduction</strong>
  </h2>
  <p xmlns="">The spatial variability found in dialects
is an essential indexical property that is highly salient to listeners in
everyday language situations: at social events, for example, one often hears conversations
of the type “I have trouble localizing your dialect – where do you come from?”.
Although listeners are typically unaware of the underlying linguistic
mechanisms involved, they are actively engaging in perceptual dialectology (cf.
Preston 1989, Clopper &amp; Pisoni 2004) and they seem keenly aware of
dialectal variation. It is interesting then that different language speaking
groups seem to recognize dialects of their language with different degrees of
accuracy. Leemann &amp; Siebenhaar (2008) and Guntern (2011) show that naïve
Swiss German listeners can accurately recognize a speaker’s dialect with a
recognition rate of 86% and 74% respectively. However, Clopper &amp; Pisoni
(2005) report identification rates of only 30–50% for American and British
English dialects; Kehrein, Lameli &amp; Purschke (2011) report similar
recognition rates for German dialects. Recent studies show that dialect
recognition is possible via the mobile application <em>Dialäkt Äpp</em> (Leemann &amp; Kolly, 2013; Kolly &amp; Leemann, in
review).</p>
  <p xmlns="">This
contribution describes work in progress: <em>Voice
Äpp</em>, currently in development at the University of Zurich, is a follow-up
project on <em>Dialäkt Äpp</em>. The main
purpose of both smartphone apps is to identify users’ dialects on the basis of
the dialectal variants of 16 words. <em>Dialäkt
Äpp</em> users provide their pronunciation through tapping on the corresponding
variant on the smartphone screen. However, the new <em>Voice Äpp</em> asks users to pronounce the word and uses automatic
speech recognition (ASR) to identify users’ pronunciation variants. The ASR
training for <em>Voice Äpp</em>is partly
based on acoustic data crowdsourced through <em>Dialäkt
Äpp</em>.<em>Voice Äpp</em>further aims at
illustrating the individuality in users’ voices by providing a multidimensional
profile of their voice. The launch of <em>Voice
Äpp</em>is planned in December 2014.</p>
  <p xmlns="">Several
research teams are interested in creating similar applications for other
languages, using the frameworks put forth by <em>Dialäkt Äpp </em>and <em>Voice Äpp</em>:
Mobile applications that recognize regional varieties of the entire
German-speaking area, of American English, of British English, and of Italian,
are currently under development.</p>
  <h2 xmlns="">
    <strong>2
Crowdsourcing data with Dialäkt Äpp</strong>
  </h2>
  <p xmlns="">In 2013 we launched the iOS application <em>Dialäkt Äpp</em>, which capitalizes on the
Swiss public interest in dialectology (Leemann &amp; Kolly, 2013). We provided a
functionality that, on the one hand, allows users to localize their own Swiss
German dialect by indicating their pronunciation of 16 words (see Figure 1). Given
the task to predict Swiss German dialects, a model was built by phoneticians
who devised a set of maximally predictive words (i.e. maps from the Linguistic
Atlas of German-speaking Switzerland: <em>Sprachatlas
der Deutschen Schweiz</em> (SDS, 1962–2003)) that capture dialectal differences
between localities. On the other hand, users can record their own dialect and
listen to recordings of other users, thus discover the Swiss dialectal
landscape. Figure 1 shows three screens of the application: the choice of dialectal
variants for the word <em>Donnerstag</em>‘Thursday’,
the identified localities as a list and on a map (Bern being the best hit in this example) and the distribution of
users’ recordings covering German-speaking Switzerland.</p>
  <div xmlns="" class="figure">
    <img src="/data/figures/DH2014/DH2014_548_Fig1_1.jpg"/>
    <figcaption>
      <p>Fig. 1: Screens of Dialäkt Äpp: (1) choice of dialectal variants with buttons; (2) result provided as a choice of five best hits and their corresponding positions on a map; (3) users’ recordings (one pin per locality)</p>
    </figcaption>
  </div>
  <p xmlns=""><em>Dialäkt
Äpp</em> was launched on
March 22, 2013, and has been downloaded over 58’000 times (as of February 28,
2013). The data recorded by this application contains (a) (written) choices of
pronunciation for 16 words by each user who localized his/her dialect and (b)
audio data for the same 16 words by each user who chose to record his/her
voice. For (a), the corpus contains data from over 42’000 subjects (58% males,
42% females). Most users are from the cantons (and capitals) of Zurich, Bern,
Basel, Luzern, Aargau, and St. Gallen. 64% of the users’ pronunciation variants
still correspond to the local variant recorded by the SDS (1962–2003) in the
1940’s and a large number of users report that the localization of their
dialect by the application is very close to their dialectal origin. For (b),
the corpus counts 38’477 recorded variants stemming from a total number of 2’633
iOS devices (which corresponds roughly to the number of speakers; 54% males, 46%
females). The geographical distribution of users corresponds to that of the
data presented in (a).</p>
  <p xmlns="">The
data elicited by <em>Dialäkt Äpp</em> has
great potential for dialectological as well as forensic phonetic research. It
can be used to create new dialect maps and compare them to the maps published
in the SDS (1962–2003), thus to track sound change in progress. A number of
maps have already been created (for the words <em>Apfelüberrest</em>‘apple core’, <em>Bett</em>
‘bed’, <em>schneien</em>‘to snow’, <em>Tanne</em>‘pine tree’, and <em>tief</em>‘low’). Preliminary analyses show
that phonetic isoglosses, as illustrated in maps like <em>Bett</em>(quality of /e/) and <em>Tanne</em>
(quanity of /n/) are congruent with data from the SDS (1962–2003) (Kolly &amp;
Leemann, in review). The data can also be used to compare dialects at the
acoustic phonetic level: For example, preliminary results show differences in
speaking rate between the Bern dialect and the Zurich dialect (Leemann, Kolly,
&amp; Dellwo, accepted). Furthermore, this corpus can be used to create
population statistics for a variety of phonetic parameters, which is desirable
for forensic phonetic voice comparison (cf. Nolan et al., 2009).</p>
  <h2 xmlns="">
    <strong>3 Development of Voice Äpp</strong>
  </h2>
  <p xmlns="">Voice Äpp has two major aims:</p>
  <p xmlns="">- To use ASR techniques to localize users’ dialects</p>
  <p xmlns="">- To provide users with a multidimensional profile of
          their voice</p>
  <h2 xmlns="">
    <strong>3.1 ASR-based dialect localization</strong>
  </h2>
  <p xmlns="">The novelty of this new project is to use
ASR techniques instead of multiple choice buttons. Some difficulties can be expected
as the ASR approach is not error-free, especially through a mobile application:
recording conditions may vary a lot due to the distance from the microphone,
noisy environments etc. However, the high-resolution microphones of
smartphones, iPhones in particular, should facilitate the ASR task. Furthermore,
identifying dialects, where small variation has to be taken into account, is
not the initial purpose of ASR systems; the speech recognition domain aims at normalizing
such variation and at being rather dialect- or speaker-independent. In addition
to this, the number of possible pronunciation variants for each word is
important. For example, the word <em>Bett</em>
‘bed’ only counts two variants in the SDS (/bet/ and /bεt/) whereas <em>Augen</em> ‘eyes’ has eleven dialectal pronunciation variants. Theß
latter is highly discriminant – but the ASR task is more difficult. The
algorithm will have to be modified since the voice recognition approach is not
as reliable as the selection with buttons.</p>
  <p xmlns="">In
order to achieve this, an ASR system is trained with two corpora: (a) the <em>Dialäkt Äpp</em> corpus described in 2 and
(b) the TEVOID corpus (Dellwo, Leemann, &amp; Kolly, 2012). Corpus (a) contains
about six hours of speech of over 2’600 speakers, covering a dense net of local
dialects in German-speaking Switzerland. Each recording is an isolated word
from a set of 16 words. Corpus (b) contains two hours and 45 minutes of speech of
16 Zurich German speakers. Each recording is either a spontaneous or a read
sentence. While the second corpus has been segmented by hand, the first one
needs data preparation and verification as it was collected without control of
linguistic content nor acoustic environment.</p>
  <p xmlns="">So
far, encouraging results are obtained with limited training data. After ASR
training with five variables from the <em>Dialäkt
Äp</em>p corpus, dialect word recognition has reached accuracies of 92% (<em>Bett</em>‘bed’), 90% (Kind ‘child’, <em>Apfelüberrest</em>‘apple
core’), 85% (<em>Tanne</em>‘fir tree’), 79% (<em>fragen</em>‘to ask’). These accuracies may
increase with larger amounts of training data, which is currently being worked
on. </p>
  <h2 xmlns="">
    <strong>3.2 Multidimensional
voice profile and infotainment content</strong>
  </h2>
  <p xmlns="">The
second function of the <em>Voice Äpp</em> is a
voice profile provided to the user. Based on a sentence recorded in their
dialect, users learn about characteristics of their own voice in a playful way.
A number of menus allow users to explore different aspects of speech, e.g.
pitch, speech rate, articulation, auditory and visual perception.</p>
  <p xmlns=""><strong><strong>Pitch</strong>:</strong> The fundamental frequency (f0)
of the users’ sentence is calculated and displayed in a histogram representing
the distribution of the f0 of all the previous users.</p>
  <p xmlns=""><strong><strong>Speech rate</strong>:</strong> The speech rate of the users’
sentence is calculated and displayed in comparison to the previous users’
speech rate.</p>
  <p xmlns=""><strong><strong>Articulation</strong>:</strong> Users learn about sounds
and their articulation. Upon clicking on an IPA symbol a sagittal cut is shown
and the sound is played. In an interactive sagittal cut users move the position
of the articulators and hear the corresponding vowel sound.</p>
  <p xmlns=""><strong><strong>Auditory perception</strong>:</strong> Users can listen
to what their sentence would sound like to a person with a hearing impairment/a
cochlear implant.</p>
  <p xmlns=""><strong><strong>Visual perception</strong>:</strong> Users are shown a video
illustrating the <em>McGurk effect</em>
(MacDonald &amp; MacGurk, 1978) and the <em>Cocktail
Party Effect</em> (Handel, 1989). Both effects illustrate that visual cues can
be crucial for speech perception.</p>
  <h2 xmlns="">
    <strong>4
Conclusion</strong>
  </h2>
  <p xmlns=""><em>Voice
Äpp</em> should be as
interactive as possible, allowing users to learn about the individual features
of their dialect and their voice in a playful way. As shown by <em>Dialäkt Äpp</em>, a mobile application such
as <em>Voice Äpp</em> is interesting for the
user as well as for the researcher: by providing appealing content to the user,
we gain large amounts of data. This crowdsourced data can be used to create
population statistics, for example for analyses of speech prosodic features. In
particular, <em>Voice Äpp </em>creates real
time f0 and speaking time statistics, which represents a novelty for e.g. the
field of forensic phonetics.</p>
  <h2 xmlns="">
    <strong>Acknowledgements</strong>
  </h2>
  <p xmlns="">The project <em>Swiss VoiceApp – Your voice. Your identity</em>
is funded by the Swiss National Science Foundation (SNSF); funding scheme:
Agora; grant number: 145654.</p>
  <h2 xmlns="">References</h2>
  <p xmlns=""><strong>Clopper, C.G., &amp; D. Pisoni</strong> (2005). <em>Perception of dialect variation</em>. In:
            Pisoni, D., R.E. Remez (Eds.), The Handbook of Speech Perception, Oxford:
            Blackwell, 313–337. </p>
  <p xmlns=""><strong>Dellwo V.,
            Leemann, A., &amp; Kolly, M.-J.</strong> (2012). <em>Speaker idiosyncratic rhythmic features
              in the speech signal</em>. Proceedings of Interspeech2012. 9.-13.9.2012, Portland (OR), USA.</p>
  <p xmlns=""><strong>Ferragne, E., &amp; Pellegrino, F. </strong>(2007). <em>Automatic
            dialect identification: A study of British English</em>. In: Speaker classification II. Berlin/Heidelberg,
            Springer: 243–257.</p>
  <p xmlns=""><strong>Guntern, M. </strong>(2011). <em>Erkennen von Dialekten anhand von
            gesprochenem Schweizerhochdeutsch</em>. Zeitschrift für Dialektologie und
            Linguistik 78/2: 155–187. </p>
  <p xmlns=""><strong>Handel, S.</strong> (1989): <em>Listening</em>.
            An Introduction to the perception of auditory events. MIT Press.</p>
  <p xmlns=""><strong>Kehrein, R., Lameli, A., &amp; Purschke, C.</strong> (2010).<em>
            Stimuluseffekte und Sprachraumkonzepte</em>. In: Anders, C., Hundt, M., Lasch A. (Eds.).
            “Perceptual dialectology”. Neue Wege der Dialektologie. Berlin/New York,
            de Gruyter: 351–384. </p>
  <p xmlns=""><strong>Leemann, A., &amp; Kolly, M.-J.</strong> (2013). <em>Dialäkt Äpp</em>.
            https://itunes.apple.com/ch/app/dialakt-app/id606559705?mt=8.</p>
  <p xmlns=""><strong>Kolly, M.-J. &amp; Leemann, A.</strong> (in review). <em>Dialäkt
            Äpp: Communicating dialectology to the public – crowdsourcing dialects from the
            public</em>. To appear in: Leemann, A., Kolly, M.-J., Schmid, S., &amp; Dellwo, V.
            (Eds.). Trends in Phonetics in German-speaking Europe, Bern/Frankfurt:
            Peter Lang.</p>
  <p xmlns=""><strong>Leemann, A., Kolly, M.-J., &amp; Dellwo, V. (</strong>accepted).
            <em>Crowdsourcing
              regional variation in speaking rate through the iOS app ‘Dialäkt Äpp’.</em> To
            appear in: Proceedings of Speech Prosody
            2014,20.–23.05.2014,
            Dublin.</p>
  <p xmlns=""><strong>Leemann,
            A., &amp; Siebenhaar, B.</strong> (2008).<em> Perception of Dialectal Prosody.</em>Proceedings
            of Interspeech 2008.</p>
  <p xmlns=""><strong>MacDonald,
            John, &amp; MacGurk, Harry </strong>(1978). <em>Visual influence on speech perception
              processes</em>. Perception &amp; Psychophysics
            24/3: 253–257.</p>
  <p xmlns=""><strong>Nolan,
            F., McDougall, K., de Jong, G., &amp; Hudson, T. </strong>(2009). <em>The DyViS database:
              style-controlled recordings of 100 homogenous speakers for forensic phonetic
              research</em>. The International Journal of
            Speech, Language and the Law 16/1: 31–57.</p>
  <p xmlns=""><em>SDS
            Sprachatlas der deutschen Schweiz</em>.
            (1962-2003). Bern (I-VI), Basel: Francke (VII-VIII).</p>
</article>
</div></section><footer><hr/></footer></body></html>